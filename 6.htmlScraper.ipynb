{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python code that extracts the date, rating and number of votes from the html pages we save and saves it in a file we specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import requests as rq\n",
    "from urllib.request import urlopen\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file path of the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLocation = input(\"Data Location: \"+\"r'\")+(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File path to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveLocation = input(\"Save Location: \"+\"r'\")+(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block of code that assigns all html files to the titles array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=[]\n",
    "entries = os.listdir(dataLocation)\n",
    "for entry in entries:\n",
    "    titles.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=len(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access to all html files is provided with two for loops. (The first allows the progress of the files in the location we selected, the other allows the progress of the content of the files).\n",
    "The working logic of the code is to decompose the html structure depending on BeatifulSoup and take the required place. Imdb's website has changed many times over the years. This means that the html structure changes. Therefore, it is necessary to control the change with if blocks that cover the date ranges in the change times. After checking the date, the data we need is taken from the necessary html tags and saved in 'csv' format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "for i in range(0,len(titles)):\n",
    "    finalurl=titles[i]\n",
    "    with open(saveLocation+finalurl+\".csv\", mode='w', newline='') as newFile:\n",
    "        newWriter = csv.writer(newFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \n",
    "        newWriter.writerow(['Year'+\";\"+'Month'+\";\"+'Day'+\";\"+'Hours'+\";\"+'Rating'+\";\"+'Vote'])\n",
    "        newFile.close()\n",
    "    filePath=os.listdir(dataLocation+finalurl[0:17])\n",
    "    for j in range(0,len(filePath)):\n",
    "            try:\n",
    "                    url=\"file:///\"+dataLocation+finalurl+\"/\"+filePath[j]\n",
    "                    page = urlopen(url);\n",
    "                    soup = bs(page, \"lxml\");\n",
    "                    dateTime=int(url[-17:-9])                     \n",
    "                    if(dateTime>=20070228 and dateTime<=20070331):\n",
    "                        try:\n",
    "                            class2 = soup.find(class_='rating')\n",
    "                            tag2 = class2.find_all('b')\n",
    "                            tag3 = class2.find_all('a')\n",
    "                            print(url[-17:-5]+\";\"+tag2[1].contents[0].replace('/10','')+\";\"+tag3[2].contents[0].replace('votes','').replace(',',''))\n",
    "                            with open(saveLocation+finalurl+\".csv\", mode='a', newline='') as newFile:\n",
    "                                newWriter = csv.writer(newFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \n",
    "                                newWriter.writerow([url[-17:-13]+\";\"+url[-13:-11]+\";\"+url[-11:-9]+\";\"+(url[-9:-7]+\":\"+url[-7:-5])+\";\"+tag2[1].contents[0].replace('/10','')+\";\"+tag3[2].contents[0].replace('votes','').replace(',','')])\n",
    "                                newFile.close()\n",
    "                        except:\n",
    "                            pass\n",
    "                    elif(dateTime>=20070401 and dateTime<=20080531):\n",
    "                        try:\n",
    "                            class2 = soup.find(class_='general rating')\n",
    "                            tag2 = class2.find_all('b')\n",
    "                            tag3 = class2.find_all('a')\n",
    "                            print(url[-17:-5]+\";\"+tag2[1].contents[0].replace('/10','')+\";\"+tag3[0].contents[0].replace('votes','').replace(',',''))\n",
    "                            with open(saveLocation+finalurl+\".csv\", mode='a', newline='') as newFile:\n",
    "                                newWriter = csv.writer(newFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \n",
    "                                newWriter.writerow([url[-17:-13]+\";\"+url[-13:-11]+\";\"+url[-11:-9]+\";\"+(url[-9:-7]+\":\"+url[-7:-5])+\";\"+tag2[1].contents[0].replace('/10','')+\";\"+tag3[0].contents[0].replace('votes','').replace(',','')])\n",
    "                                newFile.close()\n",
    "            \n",
    "                        except:\n",
    "                            pass\n",
    "                    elif(dateTime>=20080601 and dateTime<=20091231):\n",
    "                        try:\n",
    "                            class2 = soup.find(class_='meta')\n",
    "                            tag2 = class2.find_all('b')\n",
    "                            tag3 = class2.find_all('a')\n",
    "                            print(url[-17:-5]+\";\"+tag2[0].contents[0].replace('/10','')+\";\"+tag3[0].contents[0].replace('votes','').replace(',',''))\n",
    "                            with open(saveLocation+finalurl+\".csv\", mode='a', newline='') as newFile:\n",
    "                                newWriter = csv.writer(newFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \n",
    "                                newWriter.writerow([url[-17:-13]+\";\"+url[-13:-11]+\";\"+url[-11:-9]+\";\"+(url[-9:-7]+\":\"+url[-7:-5])+\";\"+tag2[0].contents[0].replace('/10','')+\";\"+tag3[0].contents[0].replace('votes','').replace(',','')])\n",
    "                                newFile.close()\n",
    "                        except:\n",
    "                            pass\n",
    "                    elif(dateTime>=20100101 and dateTime<=20100930):\n",
    "                        try:\n",
    "                            class2 = soup.find(class_='starbar-meta')\n",
    "                            tag2 = class2.find_all('b')\n",
    "                            tag3 = class2.find_all('a')\n",
    "                            print(url[-17:-5]+\";\"+tag2[0].contents[0].replace('/10','')+\";\"+tag3[0].contents[0].replace('votes','').replace(',',''))\n",
    "                            with open(saveLocation+finalurl+\".csv\", mode='a', newline='') as newFile:\n",
    "                                newWriter = csv.writer(newFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \n",
    "                                newWriter.writerow([url[-17:-13]+\";\"+url[-13:-11]+\";\"+url[-11:-9]+\";\"+(url[-9:-7]+\":\"+url[-7:-5])+\";\"+tag2[0].contents[0].replace('/10','')+\";\"+tag3[0].contents[0].replace('votes','').replace(',','')])\n",
    "                                newFile.close()\n",
    "                        except:\n",
    "                            pass\n",
    "                    elif(dateTime>=20101001 and dateTime<=20110731):\n",
    "                        try:\n",
    "                            class2 = soup.find(class_='star-box')\n",
    "                            tag2 = class2.find_all('a')\n",
    "                            tag3 = class2.find_all('span')\n",
    "                            print(url[-17:-5]+\";\"+tag3[13].contents[0]+\";\"+tag2[11].contents[0].replace(',','').replace('votes',''))\n",
    "                            with open(saveLocation+finalurl+\".csv\", mode='a', newline='') as newFile:\n",
    "                                newWriter = csv.writer(newFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \n",
    "                                newWriter.writerow([url[-17:-13]+\";\"+url[-13:-11]+\";\"+url[-11:-9]+\";\"+(url[-9:-7]+\":\"+url[-7:-5])+\";\"+tag3[13].contents[0]+\";\"+tag2[11].contents[0].replace(',','').replace('votes','')])\n",
    "                                newFile.close()\n",
    "                        except:\n",
    "                            pass\n",
    "                    elif(dateTime>=20110801 and dateTime<=20151213):\n",
    "                        try:\n",
    "                            class2 = soup.find(class_='star-box-details')\n",
    "                            tag2 = class2.find_all('span')\n",
    "                            print(url[-17:-5]+\";\"+tag2[0].contents[0]+\";\"+tag2[3].contents[0].replace(',',''))\n",
    "                            with open(saveLocation+finalurl+\".csv\", mode='a', newline='') as newFile:\n",
    "                                newWriter = csv.writer(newFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \n",
    "                                newWriter.writerow([url[-17:-13]+\";\"+url[-13:-11]+\";\"+url[-11:-9]+\";\"+(url[-9:-7]+\":\"+url[-7:-5])+\";\"+tag2[0].contents[0]+\";\"+tag2[3].contents[0].replace(',','')])\n",
    "                                newFile.close()\n",
    "                        except:\n",
    "                            pass\n",
    "                    elif(dateTime>=20151214 and dateTime<=20160106):\n",
    "                        try:\n",
    "                            class2 = soup.find(class_='imdbRating')\n",
    "                            tag2 = class2.find_all('span')\n",
    "                            print(url[-17:-5]+\";\"+tag2[0].contents[0]+\";\"+tag2[3].contents[0].replace(',',''))\n",
    "                            with open(saveLocation+finalurl+\".csv\", mode='a', newline='') as newFile:\n",
    "                                newWriter = csv.writer(newFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \n",
    "                                newWriter.writerow([url[-17:-13]+\";\"+url[-13:-11]+\";\"+url[-11:-9]+\";\"+(url[-9:-7]+\":\"+url[-7:-5])+\";\"+tag2[0].contents[0]+\";\"+tag2[3].contents[0].replace(',','')])\n",
    "                                newFile.close()\n",
    "                        except:\n",
    "                            pass\n",
    "                    elif(dateTime>=20160107 and dateTime<=20160126):\n",
    "                        try:\n",
    "                            class2 = soup.find(class_='star-box-details')\n",
    "                            tag2 = class2.find_all('span')\n",
    "                            print(url[-17:-5]+\";\"+tag2[0].contents[0]+\";\"+tag2[3].contents[0].replace(',',''))\n",
    "                            with open(saveLocation+finalurl+\".csv\", mode='a', newline='') as newFile:\n",
    "                                newWriter = csv.writer(newFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \n",
    "                                newWriter.writerow([url[-17:-13]+\";\"+url[-13:-11]+\";\"+url[-11:-9]+\";\"+(url[-9:-7]+\":\"+url[-7:-5])+\";\"+tag2[0].contents[0]+\";\"+tag2[3].contents[0].replace(',','')])\n",
    "                                newFile.close()\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    elif(dateTime>=20160127 and dateTime<=20220101):\n",
    "                        try:\n",
    "                            class2 = soup.find(class_='imdbRating')\n",
    "                            tag2 = class2.find_all('span')\n",
    "                            print(url[-17:-5]+\";\"+tag2[0].contents[0]+\";\"+tag2[3].contents[0].replace(',',''))\n",
    "                            with open(saveLocation+finalurl+\".csv\", mode='a', newline='') as newFile:\n",
    "                                newWriter = csv.writer(newFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \n",
    "                                newWriter.writerow([url[-17:-13]+\";\"+url[-13:-11]+\";\"+url[-11:-9]+\";\"+(url[-9:-7]+\":\"+url[-7:-5])+\";\"+tag2[0].contents[0]+\";\"+tag2[3].contents[0].replace(',','')])\n",
    "                                newFile.close()\n",
    "                        except:\n",
    "                             pass\n",
    "                        \n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
